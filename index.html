<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Ye Zhu | Home</title>
  <meta name="description" content="Ye Zhu">
  <meta name="author" content="Ye Zhu">
  <meta property="og:title" content="Ye Zhu" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="http://" />
  <meta property="og:site_name" content="Ye Zhu" />
  <link rel="canonical" href="http://" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img class="u-max-full-width" src='/assets/profile-pics/profile2.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>Ye Zhu</h1>
            <p>Ph.D. candidate in Computer Science, Illinois Institute of Technology </br> Visiting Ph.D. in Computer Science, Princeton University</p>
            <p>Contact: yzhu96 [AT] hawk.iit.edu</p>
            <!-- <p>https://l-yezhu.github.io/assets/cv/cv.pdf</p> -->
            <p>


<!--               <span onclick="window.open('')" style="cursor: pointer">
                <i class="fa fa-linkedin-square" aria-hidden="true"></i>
              </span> -->
              
              <span onclick="window.open('https://github.com/L-YeZhu')" style="cursor: pointer">
                <i class="fa fa-github" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://twitter.com/szyezhu')" style="cursor: pointer">
                <i class="fa fa-twitter" aria-hidden="true"></i>
              </span>
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#bio>About Me</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#news>News</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Main Research</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications2>ML4Astro</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#talk>Talks</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#tidbit>Tidbits</a></li>
        </ul>
      </div>
    </nav>

    <!-- ========== BIO ========== -->
<div class="docs-section" id="bio">
  <h4>About Me</h4>


  <p>
I am a Ph.D. student in Computer Science at Illinois Institute of Technology, and a visiting Ph.D. student in the Visual AI lab at Princeton University. </p>

<p>
My main research focuses on multimodal learning (vision, audio, and languages) and generative models, working with <a href="https://scholar.google.com/citations?user=zhi-j1wAAAAJ&hl=en" target="_blank">Prof. Yan Yan</a>, <a href="https://www.cs.princeton.edu/~olgarus/" target="_blank">Prof. Olga Russakovsky</a>, and <a href="http://yu-wu.net/" target="_blank">Prof. Yu Wu</a>. </br>
I am also interested in #AI4Science, especially in #ML4Astrophysics, which aims to solve real scientific problems using learning-based models and tools. </br>
  I serve as a reviewer for CVPR, ECCV, ICCV, NeurIPS, ICML, AAAI, IEEE Transactions on Multimedia, ACM Multimedia, ICASSP, Neurocomputing and etc.
</p>

<p>
I received my B.S. and M.S. degrees in MAE from Shanghai Jiao Tong University, China, in 2016 and 2019, respectively. During my undergrad, I received two-year intensive training in mathematics and physics under the <a href="https://en.wikipedia.org/wiki/Classe_pr%C3%A9paratoire_aux_grandes_%C3%A9coles">"French classe préparatoire"</a> system.
I also studied at Ecole Polytechnique in France and got my French Engineering diploma in 2019.
Throughout my graduate studies, I have interned several times in the industry, including Snap Inc. in the USA in 2021, and Bang & Olufsen A/S in Denmark in 2018. 
  </p>

  <p>
In addition to the academic work, I also hold French language diplomas in both <a href="https://en.wikipedia.org/wiki/Dipl%C3%B4me_approfondi_de_langue_fran%C3%A7aise#DALF_C1">DALF and TCF C1</a>, and have been a French-Chinese translator for the European scientific magazine <a href="https://fr.wikipedia.org/wiki/Science_et_Vie" target="_blank">Science & Vie</a> since 2015.
  </p>


<p>Find full <a href=/assets/cv/cv.pdf target="_blank">CV/resume</a> with last update in 04/2023.</p>

<!-- <p>

</p> -->

</div>


<!-- ========== BIO ========== -->
<div class="docs-section" id="news">
  <h4>News</h4>
  <p>
    04/2023: Our first #ML4Astro work that applies Diffusion Models to predict the density of molecular clouds has been accepted to the top astronomy venue The Astrophysical Journal (APJ). Chere <a href="https://arxiv.org/abs/2304.01670">here</a>!
  </p>
  <p>
    03/2023: Received the ACM-W scholarship and ICLR2023 financial assistance award.
  </p>
  <p>
  03/2023: One paper about Denoising Diffusion Models for astrophysics application accepted to ICLR2023 Workshop Physics4ML.
  </p>
  <p>
  01/2023: One paper about Discrete Contrastive Diffusion accepted to ICLR2023.
  </p>
  <p>
  01/2023: Received the Award of Excellence in Dissertation Research for the College of Computing at IIT.
  </p>
  <p>
  07/2022: One paper about dance-to-music cross modality generation accepted to ECCV2022.
  </p>
<!--   <p>
  06/2022: Great pleasure to join the <a href="https://visualai.princeton.edu/" target="_blank">Princeton Visual AI lab</a> as a visiting researcher, working with <a href="https://www.cs.princeton.edu/~olgarus/" target="_blank">Prof. Olga Russakovsky</a>. 
  </p>
  <p>
  05/2022: Recieved the CVPR 2022 travel grant.
  </p>
  <p>
  07/2021: One paper about vision and language accepted to TPAMI 2021.
  </p>
  <p>
  05/2021: Start research internship at Snap Inc..
  </p> -->



</div>



<!-- ========== PUBLICATIONS ========== -->
<div class="docs-section" id="publications">
  <h4>Featured Publications</h4>

  <p>Complete publications on <a href="https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1" target="_blank">Google Scholar</a>.<br/>
  </p>



  <div class="tab-content">
    <div class="tab-pane active" id="papers-selected">
      
      
        <div class="paper">
          <p class="title"><b>Boundary Guided Mixing Trajectory for Semantic Control with Diffusion Models</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2302.08357" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/BoundaryDiffusion" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Vision+X: A Survey on Multimodal Learning in the Light of Data</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Nicu Sebe, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2210.02884" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan</p>
          <p><i>International Conference on Learning Representations <b>(ICLR)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07771" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/CDCD" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Quantized GAN for Complex Music Generation from Dance Videos</b></p>
          <p><u><b>Ye Zhu</b></u>, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, and Sergey Tulyakov</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2022</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2204.00604" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/D2M-GAN" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Saying the Unseen: Video Descriptions via Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2106.14069.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Audio-Visual Correlations From Variational Cross-Modal Generations</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Hugo Latapie, Yi Yang, and Yan Yan</p>
          <p><i>IEEE International Conference on Acoustics, Speech and Signal Processing <b>(ICASSP)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2102.03424.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Learning-Audio-Visual-Correlations" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition</b></p>
          <p>Xiaoguang Zhu, <u><b>Ye Zhu</b></u>, Haoyu Wang, Honglin Wen, Yan Yan, Peilin Liu.</p>
          <p><i>Transactions on Multimedia Computing Communications and Applications <b>(TOMM)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2202.11374" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07935.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Hierarchical HMM for Eye Movement Classifications</b></p>
          <p><u><b>Ye Zhu</b></u>, Yan Yan, and  Oleg Komogortsev</p>
          <p><i>European Conference on Computer Vision Workshop <b>(ECCV Workshop)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07961.pdf" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
    </div>

    <div class="tab-pane" id="papers-all">
      
        <div class="paper">
          <p class="title"><b>Boundary Guided Mixing Trajectory for Semantic Control with Diffusion Models</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2302.08357" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/BoundaryDiffusion" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Denoising Diffusion Probabilistic Models to Predict the Number Density of Molecular Clouds in Astronomy</b></p>
          <p>Duo Xu, Jonathan Tan*, Chia-Jung Hsu, and <u><b>Ye Zhu</b></u></p>
          <p><i>International Conference on Learning Representations Physics4ML Workshop <b>(ICLR Workshop)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/forum?id=KiwRgaRYqRE" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Vision+X: A Survey on Multimodal Learning in the Light of Data</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Nicu Sebe, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2210.02884" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan</p>
          <p><i>International Conference on Learning Representations <b>(ICLR)</b>, 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07771" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/CDCD" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Quantized GAN for Complex Music Generation from Dance Videos</b></p>
          <p><u><b>Ye Zhu</b></u>, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, and Sergey Tulyakov</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2022</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2204.00604" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/D2M-GAN" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Supplementing Missing Visions via Dialog for Scene Graph Generation</b></p>
          <p><u><b>Ye Zhu</b></u>, Xiaoguang Zhu, Yuzhang Shang, Zhenhao Zhao, and Yan Yan</p>
          <p><i>arXiv preprint</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2204.11143.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/SI-Dial" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Saying the Unseen: Video Descriptions via Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2106.14069.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Audio-Visual Correlations From Variational Cross-Modal Generations</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Hugo Latapie, Yi Yang, and Yan Yan</p>
          <p><i>IEEE International Conference on Acoustics, Speech and Signal Processing <b>(ICASSP)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2102.03424.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Learning-Audio-Visual-Correlations" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition</b></p>
          <p>Xiaoguang Zhu, <u><b>Ye Zhu</b></u>, Haoyu Wang, Honglin Wen, Yan Yan, Peilin Liu.</p>
          <p><i>Transactions on Multimedia Computing Communications and Applications <b>(TOMM)</b>, 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2202.11374" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents</b></p>
          <p><u><b>Ye Zhu</b></u>, Yu Wu, Yi Yang, and Yan Yan</p>
          <p><i>European Conference on Computer Vision <b>(ECCV)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07935.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020" target="_blank">Code</a>
            
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Hierarchical HMM for Eye Movement Classifications</b></p>
          <p><u><b>Ye Zhu</b></u>, Yan Yan, and  Oleg Komogortsev</p>
          <p><i>European Conference on Computer Vision Workshop <b>(ECCV Workshop)</b>, 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2008.07961.pdf" target="_blank">Paper</a>
            

            

            

            

            
          </div>
        </div>
      
    </div>
  </div>
</div>


<div class="docs-section" id="publications2">
  <h4>ML4Astrophysics</h4>

  <p>Complete publications on <a href="https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1" target="_blank">Google Scholar</a>.<br/>
  </p>



  <div class="tab-content">
    <div class="tab-pane active" id="papers-selected">
      
      
        <div class="paper">
          <p class="title"><b>Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds</b></p>
          <p>Duo Xu, Jonathan C. Tan, Chia-Jung Hsu, and <u><b>Ye Zhu</b></u></p>
          <p><i>The Astrophysical Journal (APJ), 2023 & ICLR Physics4ML Workshop 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2302.08357" target="_blank">Main Paper</a>
            
            
              <a class="button" href="https://openreview.net/forum?id=KiwRgaRYqRE" target="_blank">Workshop</a>
            
            

            

            

            
          </div>
        </div>
      
    </div>

    
  </div>
</div>


<div class="docs-section" id="talk">
  <h4>Talks</h4>

<!--    <p>
  05/2023: Talk on Diffusion Models at Shanghai Jiao Tong University, China.
  </p>  -->


   <p>
  04/2023: Guest lecture on Diffusion Models for COS429 at Princeton University, USA.
  </p> 

   <p>
  04/2023: Talk on Diffusion Models at PIXL Lunch, Princeton University, USA.
  </p> 


   <p>
  11/2022: Talk (remote) on Multimodal Generation for Audio and Music at Bang & Olufsen, Denmark.
  </p> 


</div>


<!-- ========== RESUME ========== -->
<div class="docs-section" id="tidbit">
  <h4>Tidbits</h4>

     <p>
 My full name is 竺 烨, which shares the exact pronunciation as "Bamboo Leaf" in Chinese.
  </p> 

<p>
I enjoy reading, hiking, cooking, traveling, and taking random photos. I read books about Chinese traditional culture and history daily, such as 庄子 (Zhuangzi) and 周易 (Zhouyi), and find them especially inspiring to research works ;).
</p>


<!-- <div class="docs-section" id="template">
    Thanks to <a href="https://github.com/msaveski/www_personal" target="_blank">Martin Saveski</a> for providing the template for the web design. 


</div> -->


    <div class="footer">
      <div class="row">
        <div class="four columns">
          Ye Zhu
        </div>
        <div class="four columns">
          Contact: yzhu96 [AT] hawk.iit.edu
        </div>
        <div class="four columns">
<!--           <span onclick="window.open('')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span> -->
          <span onclick="window.open('https://github.com/L-YeZhu')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://scholar.google.com/citations?user=uk5WuyIAAAAJ&hl=en&authuser=1')" style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://twitter.com/szyezhu')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
 <!--  <script>
  // (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  // (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  // m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  // })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  // ga('create', '', 'auto');
  // ga('send', 'pageview');

</script> -->

  <!-- do not remove -->
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
